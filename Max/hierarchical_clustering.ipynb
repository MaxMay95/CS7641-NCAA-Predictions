{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Hierarchical Clustering for seeding (Max May)\n",
    "In the NCAA March Madness tournament there are 64 teams each year qualified \n",
    "to participate. These 64 Teams are grouped by a comitee into 16 seeds. \n",
    "Each seed consists 4 teams which have approximately the same performance. \n",
    "With seed number 1 containing the four best teams to seed number 16 containing \n",
    "the 4 weakest teams.\n",
    "\n",
    "The task is to cluster the teams from the tournament based on their season stats \n",
    "and then compare the clustering to the decision from the comitee.\n",
    "Hierarchical clustering is used to build a grouping with variable cluster sizes \n",
    "depending on where the tree is cut. As the distincion between 16 seeds might be too\n",
    "hard, an even number of seeds can be conflated to one and the clustering validation\n",
    "can be compared for different seed sizes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn import cluster\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster import hierarchy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Import necessary packages.\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, the data of the tournament from 2010-2018 is retrieved from a csv file and\n",
    "organized as a pandas dataframe. There is one function for getting all teams from each\n",
    "year participating in the tournament with their stats and respective seed in the\n",
    "corresponding year.\n",
    "And another function which loads all data, distinguishing only between qualified and\n",
    "non-qualified teams."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_team_data(filter_year=None,datasets_dir=None,stats_file_name=None):\n",
    "    datasets_dir = '../datasets'\n",
    "    stats_file_name = 'Stats by team and year 2010-2018.csv'\n",
    "    stats_file_path = os.path.join(datasets_dir, stats_file_name) \n",
    "    df_stats = pd.read_csv(stats_file_path, sep=',')\n",
    "    df_stats = df_stats.loc[:, ~df_stats.columns.str.contains('^Unnamed')]\n",
    "    df_stats = df_stats.loc[:, df_stats.columns.str.contains('Norm$|Year|Seed')]\n",
    "    if filter_year != None:\n",
    "        df_stats = df_stats.loc[df_stats['Year'] == filter_year,:]\n",
    "    df_stats = df_stats.loc[df_stats['Seed'] != 0,:]\n",
    "    df_stats = df_stats.drop(['Year','Seed Norm'],axis=1)\n",
    "    df_stats.reset_index(drop=True,inplace=True)\n",
    "    label = df_stats['Seed']       \n",
    "    data = df_stats.loc[:, ~df_stats.columns.str.contains('Seed')]\n",
    "    data = data.reset_index(drop=True)\n",
    "    label = label.reset_index(drop=True)\n",
    "    return data, label\n",
    "\n",
    "def adjustNumberOfSeeds(label_true,Nseeds):\n",
    "    label_adj = pd.Series.copy(label_true)\n",
    "    if Nseeds < 16:\n",
    "        n_perSeed = round(16/Nseeds)\n",
    "        for n in range(0,Nseeds):\n",
    "            label_adj.loc[((label_true>n*n_perSeed) & (label_true<=(n+1)*n_perSeed))]=n+1\n",
    "    return label_adj\n",
    "\n",
    "def get_qual_data(filter_year=None):\n",
    "    datasets_dir = '../datasets'\n",
    "    stats_file_name = 'Stats by team and year 2010-2018.csv'\n",
    "    stats_file_path = os.path.join(datasets_dir, stats_file_name) \n",
    "    df_stats = pd.read_csv(stats_file_path, sep=',')\n",
    "    df_stats = df_stats.loc[:, ~df_stats.columns.str.contains('^Unnamed')]\n",
    "    df_stats = df_stats.loc[:, df_stats.columns.str.contains('Norm$|Year|Seed')]\n",
    "    if filter_year != None:\n",
    "        df_stats = df_stats.loc[df_stats['Year'] == filter_year,:]\n",
    "    df_stats = df_stats.drop(['Year','Seed Norm'],axis=1)\n",
    "    df_stats.loc[df_stats['Seed']!=0,'Seed'] = 1\n",
    "    df_stats.reset_index(drop=True,inplace=True)\n",
    "    label = df_stats['Seed']       \n",
    "    data = df_stats.loc[:, ~df_stats.columns.str.contains('Seed')]\n",
    "    data = data.reset_index(drop=True)\n",
    "    label = label.reset_index(drop=True)\n",
    "    return data, label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Retrieve data from CSV file and filter/organize by year and change number of seeds.\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "team_stats, seeds = get_team_data(filter_year=None, datasets_dir='../datasets', stats_file_name=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Get data and labels.\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seeds.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Check for correct # of teams per seed.\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "team_stats.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Overview of features.\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "team_stats.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Histograms of features.\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Principal component analysis is used in the preprocessing to reduce the number of features\n",
    "from 14 to 9. Getting a retained variance of at least 95%."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pca = PCA(n_components=14)\n",
    "principalComponents = pca.fit_transform(team_stats)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['PC1', 'PC2', 'PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10','PC11','PC12','PC13','PC14'])\n",
    "\n",
    "percent_variance = np.round(pca.explained_variance_ratio_* 100, decimals =2)\n",
    "columns = ['PC1', 'PC2', 'PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10','PC11','PC12','PC13','PC14']\n",
    "plt.bar(x= range(1,15), height=percent_variance, tick_label=columns)\n",
    "plt.ylabel('Percentage of Variance')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.title('PCA Scree Plot')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Perform full PCA.\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "variance = 0\n",
    "dim = 1\n",
    "while variance < .95:\n",
    "    variance = np.sum(pca.explained_variance_ratio_[:dim])\n",
    "    dim = dim + 1\n",
    "print(\"We need %2d features to recover %2.1f%% of the variance.\"%(dim,variance*100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Calculate retained variance for certain # of features.\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pcaTeam_stats = principalDf.drop(['PC10','PC11','PC12','PC13','PC14'],axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Remove unnecessary features\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Through PCA the data can be visualized in 3D by the first three principal components.\n",
    "If only data from a single year is selected, one can see that it is hard to group these\n",
    "data points into clusters using only three features. If the data from all nine years is \n",
    "plotted, some patterns and clusters can be seen. But its still hard to disinguish\n",
    "between some of the overlapping seed clusters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pca3 = PCA(n_components=3)\n",
    "pC3 = pca3.fit_transform(team_stats)\n",
    "pDf3 = pd.DataFrame(data = pC3\n",
    "             , columns = ['pc1', 'pc2', 'pc3'])\n",
    "scatter3D = plt.figure().gca(projection='3d')\n",
    "scatter3D.scatter(pDf3['pc1'],pDf3['pc2'],pDf3['pc3'],c=seeds,cmap=plt.get_cmap('tab10'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% 3D visualzation of first three principal components.\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can use a dendrogram to visualize how different choices for the distance calculation\n",
    "between clusters affect the creation of the hierarchical tree. Our goal is to avoid chaining\n",
    "effects and aim for evenly sized clusters. To achieve this the average linkage is a good choice."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
    "                                      counts]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    hierarchy.dendrogram(linkage_matrix, **kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dCluster = cluster.AgglomerativeClustering(n_clusters=None,distance_threshold=0,affinity=\"euclidean\",linkage='average')\n",
    "hdistance_team = dCluster.fit(pcaTeam_stats)\n",
    "plot_dendrogram(hdistance_team,truncate_mode='level',p=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the evaluation of the cluster performance we can match the cluster numbers from the\n",
    "algorithm with the seed labels from the data set, using the contigency matrix and maximum\n",
    "weight matching.\n",
    "Now we can calculate the purity from the matched clusters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import itertools\n",
    "def get_permutations(setSize):\n",
    "    perm = list(itertools.permutations(range(setSize),setSize))\n",
    "    return np.array(perm)\n",
    "\n",
    "def max_weight_match(confMat):\n",
    "    perm = get_permutations(np.size(confMat,axis=0))\n",
    "    weightSum = np.zeros(np.size(perm,axis=0))\n",
    "    for i in range(np.size(perm,axis=0)):\n",
    "        weightSum[i] = np.sum(confMat[range(np.size(perm,axis=1)),perm[i,:]])\n",
    "    return perm[np.argmax(weightSum),:]\n",
    "\n",
    "def rematch_clusters(y_true,y_pred):\n",
    "    conMatrix = metrics.cluster.contingency_matrix(y_true,y_pred).T\n",
    "    newMatching = max_weight_match(conMatrix)\n",
    "    y_newMatch = np.zeros(y_pred.shape)\n",
    "    for i in range(len(newMatching)):\n",
    "        y_newMatch[y_pred == i] = newMatching[i]\n",
    "    return y_newMatch\n",
    "\n",
    "def matched_purity(y_true,y_pred):\n",
    "    y_pred = rematch_clusters(y_true,y_pred)\n",
    "    conMatrix = metrics.cluster.contingency_matrix(y_true,y_pred).T\n",
    "    match = max_weight_match(conMatrix)\n",
    "    purity = np.sum(conMatrix[range(np.size(conMatrix,axis=0)),match])/np.sum(conMatrix)\n",
    "    return purity"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The rows of the old contingency matrix are swapped according to the matching\n",
    "to get the new contingency matrix."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_true = adjustNumberOfSeeds(seeds,4)\n",
    "y_true = np.array(y_true)\n",
    "hCluster = cluster.AgglomerativeClustering(n_clusters=4,affinity=\"euclidean\",linkage=\"average\")\n",
    "seedCluster = hCluster.fit(pcaTeam_stats)\n",
    "y_pred = seedCluster.labels_\n",
    "\n",
    "conMatrix = metrics.cluster.contingency_matrix(y_true,y_pred).T    \n",
    "conMatrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_weight_match(conMatrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_new = rematch_clusters(y_true,y_pred)\n",
    "newConMat = metrics.cluster.contingency_matrix(y_true,y_new).T\n",
    "newConMat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%        \n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The matched purity is a little lower as each ground truth can only be matched by one cluster.\n",
    "But as the permutations increase with the factorial, computation time is significantly\n",
    "increased for 8 and 16 seed matching. Therefore, we will use the more simple computation\n",
    "of purity in the following code."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.sum(np.amax(conMatrix,axis=1))/np.sum(conMatrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Purity with masimum entry in contingency matrix.\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "matched_purity(y_true,y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Purity with weight matching.\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let us perform hierarchical clustering with full 16,8,4 and 2 seed partitioning of\n",
    "the qualified teams in the tournament."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clusterSizes = [2,4,8,16]\n",
    "purity = np.zeros(len(clusterSizes))\n",
    "randScore = np.zeros(len(clusterSizes))\n",
    "mutInfo = np.zeros(len(clusterSizes))\n",
    "for i,seedNumber in enumerate(clusterSizes):\n",
    "    y_true = adjustNumberOfSeeds(seeds,seedNumber)\n",
    "    y_true = np.array(y_true)\n",
    "    hCluster = cluster.AgglomerativeClustering(n_clusters=seedNumber\n",
    "                                               ,affinity=\"euclidean\"\n",
    "                                               ,linkage=\"average\")\n",
    "    seedCluster = hCluster.fit(pcaTeam_stats)\n",
    "    y_pred = seedCluster.labels_\n",
    "    \n",
    "    conMat = metrics.cluster.contingency_matrix(y_true,y_new).T\n",
    "    #Performance measures\n",
    "    purity[i] = np.sum(np.amax(conMat,axis=1))/np.sum(conMat)\n",
    "    print(\"Clustering in %d seeds reaches a purity of %.2f.\"%(seedNumber,purity[i]))\n",
    "    \n",
    "    randScore[i] = metrics.adjusted_rand_score(y_true,y_pred)\n",
    "    print(\"Clustering in %d seeds reaches a adjusted rand index of %.2f.\"%(seedNumber,randScore[i]))\n",
    "    \n",
    "    mutInfo[i] = metrics.adjusted_mutual_info_score(y_true,y_pred)\n",
    "    print(\"Clustering in %d seeds reaches a adjusted mutual information of %.2f.\"%(seedNumber,mutInfo[i]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Perfom hierarchical clustering for different seed sizes\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(clusterSizes,purity,label='Purity')\n",
    "plt.plot(clusterSizes,randScore,label='Adjusted Random Score')\n",
    "plt.plot(clusterSizes,mutInfo,label='Adjusted Mutual Information')\n",
    "plt.xlabel('# of Seeds/Clusters')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Cluster Validation Measures')\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The purity gives us an indicator of how purely the clusters consist of only datapoints\n",
    "from a single ground truth, with 1 corresponding to the best purity. \n",
    "For the given task we get the best purity when we only partition\n",
    "into two seeds and the purity decreases with the number of seeds, as expected.\n",
    "\n",
    "\n",
    "The mutual information measures the amount of information shared between clustering\n",
    "and ground truth. The adjusted variant of MI is independent of the number of clusters in\n",
    " a partition, otherwise a higher number of clusters would give a better MI score.\n",
    " Larger values indicate a good clustering. Our results show a slight decrease in the AMI for\n",
    " a increased number of seeds.\n",
    " \n",
    "\n",
    "The random score is a pairwise measure, which is the fraction of true positives and true \n",
    "negatives over the total number of pairs. The adjusted rand score is centered and normalized\n",
    "to adjust for chance. Negative values are bad, close to zero means random and a score of one\n",
    "means that the clusterings are identical up to label permutations.\n",
    "For our task the ARI slightly decreases with the number of seeds.\n",
    "\n",
    "\n",
    "An overall trend of decreased performance can be observed as the partitioning of the teams\n",
    "into the seeds gets finer. With a purity below 20% for partitioning into the regular 16 seeds,\n",
    "this means that this task is harder than expected based on the provided\n",
    "features. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Purity for 16 seeds is %.2f\"%(purity[3]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Qualified vs. non-qualified clustering\n",
    "Another clustering task is to determine what teams got qualified for the tournament."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "team_stats_all, qualified = get_qual_data(filter_year=None)\n",
    "qualified.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The 3D visualzation via PCA shows that the clusters of qualified vs. non-qualfied teams\n",
    "is much better to distinguish."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pca3 = PCA(n_components=3)\n",
    "pC3 = pca3.fit_transform(team_stats_all)\n",
    "pDf3 = pd.DataFrame(data = pC3\n",
    "             , columns = ['pc1', 'pc2', 'pc3'])\n",
    "scatter3D = plt.figure().gca(projection='3d')\n",
    "scatter3D.scatter(pDf3['pc1'],pDf3['pc2'],pDf3['pc3'],c=qualified,cmap=plt.get_cmap('tab10'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% 3D visualzation of first three principal components.\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is also visible in the decrease in variance of the principal components. \n",
    "Over 50% of the variance is contained in the first component, whereas the other components\n",
    "only have minor contributions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pcaAll = PCA(n_components=14)\n",
    "principalComponents = pcaAll.fit_transform(team_stats_all)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['PC1', 'PC2', 'PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10','PC11','PC12','PC13','PC14'])\n",
    "\n",
    "percent_variance = np.round(pcaAll.explained_variance_ratio_* 100, decimals =2)\n",
    "columns = ['PC1', 'PC2', 'PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10','PC11','PC12','PC13','PC14']\n",
    "plt.bar(x= range(1,15), height=percent_variance, tick_label=columns)\n",
    "plt.ylabel('Percentage of Variance')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.title('PCA Scree Plot')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Perform full PCA.\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "variance = 0\n",
    "dim = 1\n",
    "while variance < .95:\n",
    "    variance = np.sum(pcaAll.explained_variance_ratio_[:dim])\n",
    "    dim = dim + 1\n",
    "print(\"We need %2d features to recover %2.1f%% of the variance.\"%(dim,variance*100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Calculate retained variance for certain # of features.\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pcaTeam_stats_all = principalDf.drop(['PC10','PC11','PC12','PC13','PC14'],axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Remove unnecessary features\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "qualCluster = cluster.AgglomerativeClustering(n_clusters=2,affinity=\"euclidean\",linkage='average')\n",
    "qualTournament = qualCluster.fit(pcaTeam_stats_all)\n",
    "y_true = np.array(qualified)\n",
    "y_pred = qualTournament.labels_\n",
    "conMatrixQ = metrics.cluster.contingency_matrix(y_true,y_pred).T\n",
    "#Performance measures\n",
    "purityQ = np.sum(np.amax(conMatrixQ,axis=1))/np.sum(conMatrixQ)\n",
    "print(\"Qualifying clustering reaches a purity of %.2f.\"%(purityQ))\n",
    "\n",
    "randScoreQ = metrics.adjusted_rand_score(y_true,y_pred)\n",
    "print(\"Qualifying clustering reaches a adjusted rand index of %.2f.\"%(randScoreQ))\n",
    "\n",
    "mutInfoQ = metrics.adjusted_mutual_info_score(y_true,y_pred)\n",
    "print(\"Qualifying clustering reaches a AMI of %.2f.\"%(mutInfoQ))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metrics.cluster.contingency_matrix(y_true,y_pred).T"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We reach a purity of 82% for the qualified vs. non-qualfied clustering.\n",
    "As both clusters contain mainly points from one ground truth. But from the\n",
    "contigency matrix we can see that both clusters are assigned to the same ground\n",
    "truth partition. This results in a bad ARI and AMI score.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}