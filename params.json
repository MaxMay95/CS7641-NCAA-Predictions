{"name":"Cs7641-ncaa-predictions","tagline":"Final project for ML","body":"## Hierarchical Clustering for seeding (Max May)\r\nIn the NCAA March Madness tournament there are 64 teams each year qualified \r\nto participate. These 64 Teams are grouped by a committee into 16 seeds. \r\nEach seed consists 4 teams which have approximately the same performance. \r\nWith seed number 1 containing the four best teams to seed number 16 containing \r\nthe 4 weakest teams.\r\n\r\nThe task is to cluster the teams from the tournament based on their season statistics \r\nand then compare the clustering to the decision from the committee.\r\nHierarchical clustering is used to build groupings with variable cluster sizes \r\ndepending on where the tree is cut. As the distinction between 16 seeds might be too\r\nhard, an even number of seeds can be conflated to one and the clustering validation\r\ncan be compared for different seed sizes.\r\n\r\nLooking at histogram plots of the feature distributions, we can see that all 14 used features\r\nare normalized in an interval from 0 to 1.\r\n\r\n<img src=\"/images/max_datahistogram.png\" style=\"float: left margin-right: 10px;\"/>\r\n\r\nThrough PCA the data can be visualized in 3D by the first three principal components. If only data from a single year is selected, one can see that it is hard to group these data points into clusters using only three features. \r\n\r\n<img src=\"/images/max_3dpca_2018.png\" style=\"float: center margin-right: 10px;\"/>\r\n\r\nIf the data from all nine years is plotted, some patterns and clusters can be seen. But its still hard to distinguish between some of the overlapping seed clusters.\r\n\r\n<img src=\"/images/max_3dpca.png\" style=\"float: center margin-right: 10px;\"/>\r\n\r\nPrincipal component analysis tells us that at least 9 out of the 14 features are needed to achieve a retained variance of 95.1%. This can be seen in the following scree plot.\r\n\r\n<img src=\"/images/max_screeplot_seeding.png\" style=\"float: center margin-right: 10px;\"/>\r\n\r\nFor hierarchical clustering two important settings need to be considered. One is the metric, for which euclidean distance is commonly used. The other one is the linkage, which can be single link, complete link, average link, etc.. Here average link was chosen, because it is mostly recommended for general tasks, avoids chaining and clusters mostly into evenly sized groups, which is necessary for our task. The effect of this can be seen in the following dendrogram.\r\n\r\n<img src=\"/images/max_dendrogram.png\" style=\"float: center margin-right: 10px;\"/>\r\n\r\nAfter clustering is performed, we evaluate how good it is by means of different validation metrics.\r\n\r\n<img src=\"/images/max_cluster_val.png\" style=\"float: center margin-right: 10px;\"/>\r\n\r\nThe purity gives us an indicator of how purely the clusters consist of only data points from a single ground truth, with 1 corresponding to the best purity. For the given task we get the best purity when we only partition into two seeds and the purity decreases with the number of seeds, as expected.\r\n\r\nThe mutual information measures the amount of information shared between clustering and ground truth. The adjusted variant of MI is independent of the number of clusters in a partition, otherwise a higher number of clusters would give a better MI score. Larger values indicate a good clustering. Our results show a slight decrease in the AMI for a increased number of seeds.\r\n\r\nThe random score is a pairwise measure, which is the fraction of true positives and true negatives over the total number of pairs. The adjusted rand score is centered and normalized to adjust for chance. Negative values are bad, close to zero means random and a score of one means that the clusterings are identical up to label permutations. For our task the ARI slightly decreases with the number of seeds.\r\n\r\nAn overall trend of decreased performance can be observed as the partitioning of the teams into the seeds gets finer. With a purity below 20% for partitioning into the regular 16 seeds, this means that this task is harder than expected based on the provided features.\r\n\r\n\r\n### Qualified vs. non-qualified\r\n\r\nAnother clustering task is to determine what teams got qualified for the tournament.\r\nThe 3D visualization via PCA shows that the clusters of qualified vs. non-qualified teams is much better to distinguish.\r\n\r\n\r\n<img src=\"/images/max_3dpca_qual.png\" style=\"float: center margin-right: 10px;\"/>\r\n\r\nThis is also visible in the decrease in variance of the principal components. Over 50% of the variance is contained in the first component, whereas the other components only have minor contributions.\r\n\r\n<img src=\"/images/max_screeplot_qual.png\" style=\"float: center margin-right: 10px;\"/>\r\n\r\nWe need  9 features to recover 96.2% of the variance.\r\nA purity of 82% for the qualified vs. non-qualified clustering is reached. As both clusters contain mainly points from one ground truth. But from the contigency matrix \r\n\r\n| Clustering/Ground truth       | T1   | T2  |\r\n|:-----------------------------:|:----:|:---:|\r\n| C1                            | 2521 | 576 |\r\n| C2                            | 28   |   0 |\r\n\r\nwe can see that both clusters are assigned to the same ground truth partition. This results in a bad ARI and AMI score close to 0. Seems that the clusters are not properly seperable by the distance metric. Because there are far less qualified than non-qualified data points, one cluster should end up much bigger than the other one and they are not evenly sized anymore.\r\n","note":"Don't delete this file! It's used internally to help with page regeneration."}