{"name":"NCAA March Madness Predictions","tagline":"Final project for CS7641 - Machine Learning","body":"### Alejandro Da Silva, Longchao Jia, Vinod Kumar, Max May, Harrison Wilco\r\n# Background\r\n\r\nEach year (considering 2020 an outlier...) 64 teams meet in the NCAA Division I Men's Basketball Tournament. This \r\ntournament, also known as NCAA March Madness, is one the most popular sporting events in the US. Predictions of \r\ntournament games are of interest to the sports-betting community as well as college sports fans — approximately 70 \r\nmillion brackets are filled out annually attempting to predict the outcome of the NCAA tournament [1].\r\n\r\n<img src=\"images/march_madness_logo.png\"/>\r\n\r\n## Tournament Structure\r\n\r\n32 Division I conference champions receive automatic bids to the tournament, while 36 teams are selected at-large from \r\nthe remaining schools. These at-large teams are chosen by an NCAA tournament selection committee. Teams are divided into \r\nfour regions before being seeded (ranked, lower seeding should indicate higher skill) 1-16. The 68 teams given bids \r\nto the tournament are narrowed down to 64 teams after the first four play-in games, and then the field of 64 teams begin\r\na single-elimination tournament. This 64 team (63 game) tournament is what our project focused on.\r\n\r\n# Methods\r\n\r\nThis project aimed to utilize various machine learning techniques to analyze the NCAA tournament. Unless otherwise \r\nnoted, the algorithm implementations used were from the Python package scikit-learn [2]. Unsupervised clustering was \r\nemployed in an attempt to group teams based on skill level (and thus replicate seeding) based on regular season \r\nstatistics from the teams. Additionally, clustering was employed to distinguish teams which qualified for the tournament\r\nfrom teams which did not. Supervised learning techniques [3] aimed to predict the winners of individual matchups in \r\ntournament play. In the past, machine learning models have proven somewhat successful in this application, yielding \r\nbetter results than predictions by sports experts [4].\r\n\r\nThe clustering approach to seeding/tournament qualification has not shown up in our prior work searches.\r\n\r\n## Data\r\n\r\nThe dataset used in this project was scraped from Sports Reference [5]. Data from the 2010 through 2018 seasons were \r\nused. Regular season statistics such as team field goal percentage, win-loss percentage, and strength of schedule were \r\nused. Additionally, win-loss percentage times strength of schedule was used as a derived statistic, as we would expect\r\nwins against a tougher schedule to be more important than wins against easy opponents. This resulted in a final set of \r\n15 features. Finally, each statistic was normalized relative to the teams which made the tournament for a given year. \r\nFor example, each team which had the worst field goal percentage of the year while making the tournament was given a 0 \r\nfor FG% Norm, while the team which made the tournament with the best FG% was given a 1 for FG% Norm. This step was taken\r\nto try to equalize the statistics over time, as averages for different statistics can vary by large amounts over a \r\nnine-year period as the game of Division I basketball changes. The use of statistics normalized by year was a new \r\napproach meant to make it easier to generalize results generated from previous years to future years.\r\n\r\nThis stats dataset was used in tandem with Sports Reference's history of past NCAA tournament games and results to \r\ncreate a combined dataset, where each instance represented one tournament matchup. For a matchup, the one of the teams\r\nwas randomly selected as Team1, while the other was Team2. The resulting features were created from the relative \r\ndifference of each statistic, that is \\[(Team1 val - Team2 val)/avg(Team1 val, Team2 val)] for for each stat from the \r\nstats dataset. The label for each instance was whether Team1 won the game.\r\n\r\n### Full list of features:\r\n\r\n1. Win-loss percentage.\r\n2. Strength of schedule (SOS).\r\n3. Offensive Rating (ORtg).\r\n4. Defensive Rating (DRtg).\r\n5. Field goal percentage.\r\n6. Seed (0 for non-qualified teams).\r\n7. Win-loss percentage times SOS.\r\n8. Three point percentage.\r\n9. Free throw percentage.\r\n10. Points per game (PPG).\r\n11. Opponents points per game (OPPG).\r\n12. Assists per game (APG).\r\n13. Turnovers per game (TOVPG).\r\n14. Adjusted margin.\r\n15. Adjusted win-loss percentage.\r\n\r\n## Evaluation\r\n\r\nThe unsupervised clustering methods were evaluated based on a number of metrics, including purity, mutual information \r\nscore, and contingency matrices.\r\n\r\nThe supervised methods were evaluated on accuracy, Matthew's correlation coefficient, and ROC AUC. Additionally, \r\nconfusion matrices are presented.\r\n\r\n# Principal Component Analysis\r\n\r\nPCA was applied before using certain methods that are best used with fewer features. Additionally, PCA was used prior to\r\napplying methods that require linearly independent features (as the base features were dependent in some cases).\r\n\r\nWe observed that out of the 15 features in the matchup data set, **70.2%** of the variance could be captured by the first three \r\nprincipal components. Furthermore, **95%** of the variance could only be captured by using at least nine components, \r\nwith **99%** of variance recovered requiring 11 components.\r\n\r\n<img src=\"images/Supervised_PCA_Scree_Plot.png\"/>\r\n\r\n# Unsupervised Learning\r\n\r\nFor the unsupervised learning, the seed feature was removed from the stats dataset (for obvious reasons). After PCA on \r\nthe stats dataset with the seed feature removed (for obvious reasons), we needed nine components to recover at least \r\n**95%** of the variance.\r\n\r\n<img src=\"images/max_screeplot_seeding.png\" style=\"float: center margin-right: 10px;\"/>\r\n\r\nIt is important to note that the dataset used in unsupervised learning is not the same as the one used for supervised \r\nlearning — the stats dataset used here was not combined with the NCAA tournament games dataset, thus each instance \r\nrepresents a Division I NCAA basketball team and their regular season statistics.\r\n\r\n## k-means\r\n\r\nAs we mentioned before, the NCAA tournament divides the 64 qualified teams into 16 different seed, according to their \r\nperformance during the regular season. In reality, the selection of tournament teams as well as seeding has a high \r\ndegree of subjectivity. Our goals are to quantify that subjectivity using the k-means clustering algorithm.\r\n\r\nWe decided to start with the data from year 2018. Initially, we sought to separate the teams into two separate clusters:\r\none for qualified teams and the other for unqualified ones using the k-means algorithm. We used all 14 principal \r\ncomponents (after excluding seeding) when computing the Euclidean distance in the algorithm.\r\n\r\n<img src=\"images/Regular_KMeans_AllTeams_2018.png\"/>\r\n<img src=\"images/AllTeams_PCA_2018.png\"/>\r\n\r\nEncouraged by the clear separation between the clusters, we decided to divide the qualified teams for the year 2018 by \r\nclustering the teams into 16 clusters (equivalent to 16 seeds) based on the all of the transformed statistics obtained \r\nwith PCA. We obtained the following results\r\n\r\n<img src=\"images/Regular_KMeans_2018_16Seeds.png\"/>\r\n<img src=\"images/Qualified_PCA_2018_16Seeds.png\"/>\r\n    \r\nIt seems like clustering was not very good. Specifically, the purity of the clustering hovers at **39.5%**. We \r\nhypothesized that decreasing the number of clusters to 4 (which corresponds into 1-4 seeds, 5-8 seeds, etc.) would yield\r\nbetter results\r\n\r\n<img src=\"images/Regular_KMeans_2018_4Seeds.png\"/>\r\n<img src=\"images/Qualified_PCA_2018_4Seeds.png\"/>\r\n    \r\nHere, purity was **56.1%**. Even though we achieved better results, we noticed another problem with the regular k-means \r\nalgorithm, specifically that number of points per cluster is not limited. For example, the number of datapoints in the \r\nlast run were as follows:\r\n\r\n| Cluster Number       | Number of Elements   |\r\n|:-----------------------------:|:----:|\r\n| 1                            | 10   |\r\n| 2                            | 15   |\r\n| 3                            | 17   |\r\n| 4                            | 22   |\r\n\r\nWe would expect to see an equal number of teams assigned to each seed (or group of seeds) if following real seeding \r\nprocedures.\r\n\r\n### Modified k-means\r\n\r\nGiven these limitations, we created a modified k-means algorithm that evenly divides datapoints across clusters. \r\nSpecifically, the clusters prioritize having the minimum distance between a datapoint and the cluster center, discarding\r\nthe points that are farther away. These points in turn are forced to belong to another cluster. The implementation \r\nlooked as follows for both the cases of 16 and 4 clusters.\r\n\r\n<img src=\"images/Modified_KMeans_2018_16Seeds.png\"/>\r\n<img src=\"images/Modified_KMeans_2018_4Seeds.png\"/>\r\n    \r\nWhen we looked at the purity of each clustering, we obtained slightly better results when compared to the regular \r\nk-means algorithm. Specifically, we obtained a purity of **41.7%** for 16 seeds (compared to 39.5%) and **57.5%** \r\n(compared to **56.1%**).\r\n\r\nWe then decided to further test the performance of both clustering algorithms by using data for all available years in \r\nthe dataset (2010-2018). We used the normalized statistics to minimize differences in performance across the years. \r\nResults of PCA are shown here for contrast against the clusters later on.\r\n\r\n<img src=\"images/Qualified_PCA_All_16Seeds.png\"/>\r\n<img src=\"images/Qualified_PCA_All_4Seeds.png\"/>\r\n    \r\nFurthermore, these are the results of running both algorithms for 16 clusters... \r\n\r\n<img src=\"images/Regular_KMeans_All_16Seeds.png\"/>\r\n<img src=\"images/Modified_KMeans_All_16Seeds.png\"/>\r\n    \r\n...and 4 clusters ...\r\n\r\n<img src=\"images/Regular_KMeans_All_4Seeds.png\"/>\r\n<img src=\"images/Modified_KMeans_All_4Seeds.png\"/>\r\n    \r\nIn terms of purity, we obtained the following measurements.\r\n\r\n| Number of Clusters       | Regular   | Modified  |\r\n|:-----------------------------:|:----:|:---:|\r\n| **16**                            | 26.5% | 26.1% |\r\n| **4**                            | 54.9%   |   55.0% |\r\n\r\n\r\nIt is important to recognize the limitations of the latter approach to unsupervised clustering. Given more data, it \r\nappears that both algorithms have the same performance. This spans from the prioritization of assignment of data points \r\nto a cluster. A careful observer would note that some points in the modified k-means algorithm are far away from their \r\ncluster center. This points could be called \"outliers\", not being close enough to any cluster center and thus assigned \r\nby default to a the center that is closest to it that doesn't have all points assigned to it yet. This method showed \r\nsome promise, but could use refinement.\r\n\r\n## Hierarchical Clustering (Seeding)\r\n\r\nHierarchical clustering is used to build groupings with variable cluster sizes depending on where the tree is cut. As \r\nthe distinction between 16 seeds might be too hard, an even number of seeds can be conflated to one and the clustering \r\nvalidation can be compared for different seed sizes. PCA was applied, keeping nine components.\r\n\r\nFor hierarchical clustering two important settings need to be considered. One is the metric, for which Euclidean \r\ndistance is commonly used. The other one is the linkage, which can be single link, complete link, average link, etc. \r\nHere, average link was chosen because it is often recommended for general tasks, avoids chaining, and clusters mostly \r\ninto evenly sized groups, which is necessary for our task. The effect of this can be seen in the following dendrogram.\r\n\r\n<img src=\"images/max_dendrogram.png\" style=\"float: center margin-right: 10px;\"/>\r\n\r\nAfter clustering is performed, we evaluate how good it is by means of different validation metrics.\r\n\r\n<img src=\"images/max_cluster_val.png\" style=\"float: center margin-right: 10px;\"/>\r\n\r\nThe purity gives us an indicator of how purely the clusters consist of only data points from a single ground truth, with\r\n1 corresponding to the best purity. For the given task we get the best purity when we only partition into two seeds and \r\nthe purity decreases with the number of seeds, as expected.\r\n\r\nThe mutual information measures the amount of information shared between clustering and ground truth. The adjusted \r\nvariant of MI is independent of the number of clusters in a partition, otherwise a higher number of clusters would give \r\na better MI score. Larger values indicate a good clustering. Our results show a slight decrease in the AMI for an \r\nincreased number of seeds.\r\n\r\nThe random score is a pairwise measure, which is the fraction of true positives and true negatives over the total number\r\nof pairs. The adjusted rand score is centered and normalized to adjust for chance. Negative values are bad, close to \r\nzero means random and a score of one means that the clustering assignments are identical up to label permutations. For \r\nour task the ARI slightly decreases with the number of seeds.\r\n\r\nAn overall trend of decreased performance can be observed as the partitioning of the teams into the seeds gets finer. \r\nWith a purity below **20%** for partitioning into the regular 16 seeds, this means that this task is harder than \r\nexpected based on the provided features.\r\n\r\n\r\n## Hierarchical Clustering (Tournament Qualification)\r\n\r\nAnother clustering task is to determine which teams qualified for the tournament.\r\nThe 3D visualization via PCA applied on the full dataset (not just qualified teams) shows that the clusters of qualified\r\nvs. non-qualified teams appears easier to distinguish.\r\n\r\n<img src=\"images/max_3dpca_qual.png\" style=\"float: center margin-right: 10px;\"/>\r\n\r\nThis is also visible in the decrease in variance of the principal components (when including all teams, not only \r\nqualified teams in PCA). Over **50%** of the variance is contained in the first component, whereas the other components \r\nonly have minor contributions.\r\n\r\n<img src=\"images/max_screeplot_qual.png\" style=\"float: center margin-right: 10px;\"/>\r\n\r\nWe need  9 features to recover **96.2%** of the variance.\r\nA purity of **82%** for the qualified vs. non-qualified clustering is reached, as both clusters contain mainly points \r\nfrom one ground truth (qualified or non-qualified). But from the contigency matrix \r\n\r\n| Clustering/Qualification       | NQ   | Q  |\r\n|:-----------------------------:|:----:|:---:|\r\n| **C1**                            | 2521 | 576 |\r\n| **C2**                            | 28   |   0 |\r\n\r\nwe can see that both clusters are assigned to the same ground truth partition. This results in a bad ARI and an AMI \r\nscore close to 0. These results suggest that the qualified and non-qualified teams are not properly separable through \r\nhierarchical clustering and the distance metric utilized. The heavy bias toward non-qualified teams (as only 64 teams \r\nmake the tournament each year) may be a contributing factor of this result.\r\n\r\n## Unsupervised Learning Conclusion\r\n\r\nThe results from both k-means and hierarchical clustering seem to highlight the poor performance of unsupervised \r\nlearning to predict the seeding positions for any given year. More complicated algorithms or more exhaustive data could \r\nyield to better results, but we expect those to only show marginal improvements over the algorithms described here given\r\nthe subjectivity of seeding assignment carried out by a group of judges. This performance could indicate one of two \r\ndifferent results (or a mix of the two):\r\n\r\n1. Clustering using the selected features is a poor way to group teams based on regular season performance.\r\n2. The selection committee does not accurately seed teams based on objective metrics of regular season performance.\r\n\r\nIt is also worth noting that since 32 teams automatically qualify to the tournament based on conference championships, \r\nsome weaker teams that happen to play in weaker conferences may qualify over stronger teams. Finally, seeding must take\r\ninto account considerations other than just strength [6]. For example, repeats of regular season matchups are avoided to\r\nsome extent in early rounds.\r\n\r\n# Supervised Results\r\n\r\nThe following results were obtained using an 80-20 train-test split. 10-fold cross validation was performed to \r\ndetermine hyperparameters for each model using ROC AUC as the metric. The cross validations plots show performance vs\r\nhyperparameter values. The blue line shows CV performance, with the shaded region showing an 80% confidence interval for \r\nthe mean performance. The orange line shows performance over the entire training set. \r\n\r\n## k-NN\r\n\r\nA standard scaler followed by PCA (with retained variance >= 0.99) were applied. 11 components are kept after the PCA \r\nstep.\r\n\r\n<img src=\"images/longchao_knn.png\"/>\r\n\r\nThe results indicate that validation ROC AUC flattens off around k=19. k=19 produced a mean CV ROC AUC of **0.784**. The\r\nplot shows clear overfitting at small values of k, as training performance is significantly higher than CV performance.\r\n\r\n## SVM\r\n\r\nSupport vector machines use a hyperplane to separate classes, potentially using a kernel trick before separation to\r\nallow for non-linear classification. We landed on a radial basis function (rbf) kernel. Two important hyperparameters \r\nfor support vector machines are C (regularization parameter) and gamma (kernel coefficient).\r\n\r\n<img src=\"images/longchao_svm_C.png\" style=\"float: left margin-right: 10px;\"/>\r\n<img src=\"images/longchao_svm_gamma.png\" style=\"float: left margin-right: 10px;\"/>\r\n\r\n## Logistic Regression\r\n\r\nA logistic regression uses a logistic function to model a binary dependent variable, resulting in a linear classifier. \r\nThe hyperparameters to be tuned are C (inverse of the regularization strength) and tol (tolerance for stopping \r\ncriteria). \r\n\r\n<img src=\"images/longchao_lr_C.png\" style=\"float: left margin-right: 10px;\"/>\r\n<img src=\"images/longchao_lr_tol.png\" style=\"float: left margin-right: 10px;\"/>\r\n\r\nThe best fit parameter we have for SVM is C=100 and tol=1e-3, yielding a mean CV ROC AUC of **0.781**. One concerning\r\naspect of the plots is the consistent underfitting evidenced. Training performance seems to be far lower than CV \r\nperformance across all hyperparameter values tested.\r\n\r\n## Decision Tree\r\n\r\nDecision trees repeatedly split data based on thresholds for a certain features to classify datapoints at leaf nodes.\r\nDecision trees trained on data before scaling and PCA performed significantly better than after PCA, thus PCA was not\r\napplied for these models. The main hyperparameter of interest is the tree's maximum depth.\r\n\r\n<img src=\"images/dt_max_depth.png\"/>\r\n\r\nAfter cross-validation, a max-depth of three was selected, yielding a mean CV ROC AUC of **0.786**. It is clear that as \r\nthe depth of the tree increases, the model tends to overfit, as training performance steadily rises while CV performance\r\nfalls off after a max depth of three. Here is a tree of depth three trained on the entire training set:\r\n\r\n<img src=\"images/dt_visualization.png\"/>\r\n\r\n## Random Forest\r\n\r\nA random forest classifier is an ensemble classifier that fits many simple decision trees to the data then has them each\r\nvote for a class label. The implementation chosen uses bagging (bootstrap aggregating). Again, PCA seemed to hurt model \r\nperformance and thus was not used.\r\n\r\n<img src=\"images/rf_n_estimators.png\"/>\r\n\r\nHere it appears CV performance levels off after approximately 60 estimators, with a mean CV ROC AUC of **0.799**. This\r\nmodel, however, appears to have extreme overfitting issues. Training ROC AUC (and accuracy) quickly hits **1.000**. To \r\ncombat this issue we decided to decrease the complexity of the base estimators by limiting the max depth of the decision\r\ntrees used to four.\r\n\r\n<img src=\"images/rf_n_estimators_simple.png\"/>\r\n\r\nNow we find 40 estimators provides the best perofrmance, with a mean CV ROC AUC of **0.814**. While this increase is\r\nlikely not statistically significant with any reasonable degree of certainty, we did achieve the desired result — \r\ntraining performance is far closer to CV performance.\r\n\r\n## MLP\r\n\r\nA multilayer perceptron is a feedforward artificial neural network (ANN) composed of multiple layers of perceptrons with\r\nthreshold activations. MLPs are relatively complex in terms of hyperparameters to tune.\r\n\r\nIn the scikit-learn implementation, we tuned the number of hidden layers, the number of neurons in each layer, alpha (L2\r\npenalty parameter), initial learning rate, hidden layer activation function, and max number of iterations. With so many \r\nhyperparameters, a grid search based approach similar to the approaches used previously would either cover too little of\r\nthe hyperparameter space, or take too long. Instead, a random search over the hyperparameter space was used. We limited \r\nthe MLP to at most two hidden layers (no performance increases seen after two layers), limited neurons per layer to the\r\nrange [5, 200], limited alpha to the range [1e-5, 1e-1] (with a log-uniform distribution), limited initial learning rate\r\nto the range [1e-4, 1e-2] (with a log-uniform distribution), and limited the max number of iterations to the range \r\n[25, 500]. Then 10000 hyperparameter configurations were run through 5-fold cross validation. The best model \r\nconfigurations from each set of 2000-3000 configurations consistently reached mean CV ROC AUC scores of approximately \r\n**0.86**. The chosen configuration had the highest score by a small margin, with a mean CV ROC AUC of **0.862**.\r\n\r\n| Hyperparameter | Value |\r\n|:--------------:|:-----:|\r\n| num hidden layers | 2 |\r\n| hidden layer sizes | (86, 24) |\r\n| initial learning rate | 3e-4 |\r\n| max iterations | 244 |\r\n| activation | tanh | \r\n\r\nWe also examined the learning rate of the selected model.\r\n\r\n<img src=\"images/mlp_learning_curves.png\"/>\r\n\r\n## Supervised Learning Conclusion\r\n\r\n### Model Comparison\r\n\r\n| Model | Mean CV ROC AUC |\r\n|:-----:|:---------------:|\r\n| k-NN          | 0.784 |\r\n| SVM           | 0.781 |\r\n| Decision Tree | 0.786 |\r\n| Random Forest | 0.814 |\r\n| MLP           | 0.862 |\r\n\r\nAs indicated above, the MLP model performed at the highest rate, with the random forest performing significantly lower. \r\nThe remaining models each had similar, even lower performances (though these ROC AUC values were still rather high).\r\n\r\n### Testing Performance\r\n\r\nSince the MLP had the highes mean CV ROC AUC, it was selected as the model to train on the entire training set and use \r\nfor testing. It achieved the following results on the testing set:\r\n\r\n| Metric | Score |\r\n|:------:|:-----:|\r\n| ROC AUC  | 0.888 |\r\n| Accuracy | 0.886 |\r\n| MCC      | 0.775 |\r\n| F1 Score | 0.885 |\r\n\r\nUsing the Wilson score interval [7], we can say with **95%** certainty that our model's mean accuracy is \r\n**0.886 +- 0.059**.\r\n\r\nThe confusion matrix produced was:\r\n\r\n| Team1 Prediction/Team1 Result | True Win | True Loss |\r\n|:-----------------------------:|:--------:|:---------:|\r\n| Predicted Win  | 51 | 9  |\r\n| Predicted Loss | 4  | 50 |\r\n\r\n### Performance Evaluation\r\n\r\nThe official NCAA Bracket Challenge game receives millions of entries each year [8]. Unfortunately, the uncertain nature of \r\nmatchups makes it hard to compare our model to overall accuracy in bracket challenge games. After the first round, the \r\nteams people predict to win games may not even get a chance to play in those games (if they were eliminated). As such,\r\nwe can only compare accuracy with first round accuracy, however this should be a reasonable comparison. From 2015-2017, \r\nfirst round games were predicted correctly **72.1%** of the time [9], a mark well below the testing accuracy of our model. \r\nAdditionally, of all first round matchups (1 vs. 16, 2 vs. 15, etc.) participants only predicted matchups of 1 vs. 16 \r\nteams at a higher rate than our model predicted all matchups (**97.9%** of 1 vs. 16 matchups were predicted correctly) \r\n[9]. 2 vs. 15 matchups were predicted at the same rate as our model's test, and matchups closer than 2 vs. 16 fell \r\nsharply in accuracy [9].\r\n\r\nPerfect predictions of NCAA tournament games will likely never be possible. Too many confounding variables are involved, \r\nfrom player injuries to motivation. Our team is very happy with the performance achieved by our final model, however. \r\nThe model scored quite well in each metric when run on the withheld test set.\r\n\r\n# Future Work\r\n\r\n## Data\r\n\r\nThe most obvious place to improve is the aquisition of more data points. While some of the advanced statistics used as \r\nfeatures were not tracked until 2010, expanding the dataset to include all NCAA tournament games but reducing features \r\ndown to basic features may still improve performance. Otherwise, as more NCAA tournament occur, more data will become \r\navailable for training.\r\n\r\nFirst, a larger number of statistics could be pulled from Sports Reference as well as other sources. Metrics such as SRS\r\n(simple rating system), pace, RPI (rating percentage index), and true shooting percentage could all provide more \r\ninformative power to models. Additionally, the NCAA has recently implemented a new metric called the NET (NCAA \r\nevaluation tool) which they claim serves as a comprehensive measure of team strength [10]. The NET ranking could not be\r\nused as NET rankings only date back to the 2019 season.\r\n\r\nA different approach entirely would be to use more in-depth, play-by-play statistics rather than aggregate season \r\nstatistics. While data collection and processing would be more difficult, model performance may see benefits. This \r\napproach may also allow the model to account for injuries or otherwise inactive players.\r\n\r\n## Supervised Learning\r\n\r\nUtilizing random searches for all models tried may have marginally increase some of their performances by testing out a\r\nlarger/more comprehensive hyperparameter space. In general, random search has several advantages over grid search:\r\n\r\n1. Adding hyperparameters which are uninformative does not decrease search efficiency [11].\r\n2. A budget can be chosen independent of the number of hyperparameters and possible values [11].\r\n3. Random search provides higher effective dimensionality, resulting in more efficient searches [12].\r\n\r\n# References\r\n[1] American Gaming Association (2019, Mach 18). *2019 March Madness Betting Estimates*.\r\n\r\n[2] scikit-learn. *scikit-learn: About us*. (n.d.). Retrieved from https://scikit-learn.org/stable/about.html.\r\n\r\n[2] Kvam, P., and J. S. Sokol. (2006, July 14) *A logistic regression/Markov chain model for NCAA basketball*. Naval \r\nResearch Logistics 53(8):788–803.\r\n\r\n[4] Bunker, R. P., and F. Thabtah. (2017, September 19) *A machine learning framework for sport result prediction*. \r\nApplied Computing and Informatics 15(1):27–33.\r\n\r\n[5] Sports Reference. *College Basketball Statistics and History: College Basketball at Sports.* (n.d.). Retrieved from \r\nhttps://www.sports-reference.com/cbb/.\r\n\r\n[6] NCAA.com. (2019, October 31). *How the field of 68 teams is picked for March Madness.* Retrieved from \r\nhttps://www.ncaa.com/news/basketball-men/article/2018-10-19/how-field-68-teams-picked-march-madness\r\n\r\n[7] Wilson, E. B. (1927). *Probable Inference, the Law of Succession, and Statistical Inference.* Journal of the \r\nAmerican Statistical Association, 22(158), 209–212. doi: 10.1080/01621459.1927.10502953\r\n\r\n[8] Wilco, D. (2020, March 26). *The absurd odds of a perfect NCAA bracket.* Retrieved from \r\nhttps://www.ncaa.com/news/basketball-men/bracketiq/2020-01-15/perfect-ncaa-bracket-absurd-odds-march-madness-dream\r\n\r\n[9] Wilco, D. (2020, April 21). Personal interview.\r\n\r\n[10] NCAA.com. (2019, December 16). *The NET, explained: NCAA adopts new college basketball ranking.* Retrieved from \r\nhttps://www.ncaa.com/news/basketball-men/article/2018-11-26/net-explained-ncaa-adopts-new-college-basketball-ranking\r\n\r\n[11] NCAA.com. *Tuning the hyper-parameters of an estimator.* (n.d.). Retrieved from \r\nhttps://scikit-learn.org/stable/modules/grid_search.html#randomized-parameter-search\r\n\r\n[12] Bergstra, J., & Bengio, Y. (2012). *Random Search for Hyper-Parameter Optimization.* Journal of Machine Learning \r\nResearch, 13, 281–305. Retrieved from http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf\r\n","note":"Don't delete this file! It's used internally to help with page regeneration."}