{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Alejandro Da Silva, Longchao Jia, Vinod Kumar, Max May, Harrison Wilco\n",
    "# Background\n",
    "\n",
    "Each year (considering 2020 an outlier...) 64 teams meet in the NCAA Division I Men's Basketball Tournament. This \n",
    "tournament, also known as NCAA March Madness, is one the most popular sporting events in the US. Predictions of \n",
    "tournament games are of interest to the sports-betting community as well as college sports fans — approximately 70 \n",
    "million brackets are filled out annually attempting to predict the outcome of the NCAA tournament [1].\n",
    "\n",
    "<img src=\"images/march_madness_logo.png\"/>\n",
    "\n",
    "## Tournament Structure\n",
    "\n",
    "32 Division I conference champions receive automatic bids to the tournament, while 36 teams are selected at-large from \n",
    "the remaining schools. These at-large teams are chosen by an NCAA tournament selection committee. Teams are divided into \n",
    "four regions before being seeded (ranked, lower seeding should indicate higher skill) 1-16. The 68 teams given bids \n",
    "to the tournament are narrowed down to 64 teams after the first four play-in games, and then the field of 64 teams begin\n",
    "a single-elimination tournament. This 64 team (63 game) tournament is what our project focused on.\n",
    "\n",
    "# Methods\n",
    "\n",
    "This project aimed to utilize various machine learning techniques to analyze the NCAA tournament. Unless otherwise \n",
    "noted, the algorithm implementations used were from the Python package scikit-learn [2]. Unsupervised clustering was \n",
    "employed in an attempt to group teams based on skill level (and thus replicate seeding) based on regular season \n",
    "statistics from the teams. Additionally, clustering was employed to distinguish teams which qualified for the tournament\n",
    "from teams which did not. Supervised learning techniques [3] aimed to predict the winners of individual matchups in \n",
    "tournament play. In the past, machine learning models have proven somewhat successful in this application, yielding \n",
    "better results than predictions by sports experts [4].\n",
    "\n",
    "The clustering approach to seeding/tournament qualification has not shown up in our prior work searches.\n",
    "\n",
    "## Data\n",
    "\n",
    "The dataset used in this project was scraped from Sports Reference [5]. Data from the 2010 through 2018 seasons were \n",
    "used. Regular season statistics such as team field goal percentage, win-loss percentage, and strength of schedule were \n",
    "used. Additionally, win-loss percentage times strength of schedule was used as a derived statistic, as we would expect\n",
    "wins against a tougher schedule to be more important than wins against easy opponents. This resulted in a final set of \n",
    "15 features. Finally, each statistic was normalized relative to the teams which made the tournament for a given year. \n",
    "For example, each team which had the worst field goal percentage of the year while making the tournament was given a 0 \n",
    "for FG% Norm, while the team which made the tournament with the best FG% was given a 1 for FG% Norm. This step was taken\n",
    "to try to equalize the statistics over time, as averages for different statistics can vary by large amounts over a \n",
    "nine-year period as the game of Division I basketball changes. The use of statistics normalized by year was a new \n",
    "approach meant to make it easier to generalize results generated from previous years to future years.\n",
    "\n",
    "This stats dataset was used in tandem with Sports Reference's history of past NCAA tournament games and results to \n",
    "create a combined dataset, where each instance represented one tournament matchup. For a matchup, the one of the teams\n",
    "was randomly selected as Team1, while the other was Team2. The resulting features were created from the relative \n",
    "difference of each statistic, that is \\[(Team1 val - Team2 val)/avg(Team1 val, Team2 val)] for for each stat from the \n",
    "stats dataset. The label for each instance was whether Team1 won the game.\n",
    "\n",
    "### Full list of features:\n",
    "\n",
    "1. Win-loss percentage.\n",
    "2. Strength of schedule (SOS).\n",
    "3. Offensive Rating (ORtg).\n",
    "4. Defensive Rating (DRtg).\n",
    "5. Field goal percentage.\n",
    "6. Seed (0 for non-qualified teams).\n",
    "7. Win-loss percentage times SOS.\n",
    "8. Three point percentage.\n",
    "9. Free throw percentage.\n",
    "10. Points per game (PPG).\n",
    "11. Opponents points per game (OPPG).\n",
    "12. Assists per game (APG).\n",
    "13. Turnovers per game (TOVPG).\n",
    "14. Adjusted margin.\n",
    "15. Adjusted win-loss percentage.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "The unsupervised clustering methods were evaluated based on a number of metrics, including purity, mutual information \n",
    "score, and contingency matrices.\n",
    "\n",
    "The supervised methods were evaluated on accuracy, Matthew's correlation coefficient, and ROC AUC. Additionally, \n",
    "confusion matrices are presented.\n",
    "\n",
    "# Principal Component Analysis\n",
    "\n",
    "PCA was applied before using certain methods that are best used with fewer features. Additionally, PCA was used prior to\n",
    "applying methods that require linearly independent features (as the base features were dependent in some cases).\n",
    "\n",
    "We observed that out of the 15 features in the matchup data set, **70.2%** of the variance could be captured by the first three \n",
    "principal components. Furthermore, **95%** of the variance could only be captured by using at least nine components, \n",
    "with **99%** of variance recovered requiring 11 components.\n",
    "\n",
    "<img src=\"images/Supervised_PCA_Scree_Plot.png\"/>\n",
    "\n",
    "# Unsupervised Results\n",
    "\n",
    "For the unsupervised learning, the seed feature was removed from the stats dataset (for obvious reasons). After PCA on \n",
    "the stats dataset with the seed feature removed (for obvious reasons), we needed nine components to recover at least \n",
    "**95%** of the variance.\n",
    "\n",
    "<img src=\"/images/max_screeplot_seeding.png\" style=\"float: center margin-right: 10px;\"/>\n",
    "\n",
    "It is important to note that the dataset used in unsupervised learning is not the same as the one used for supervised \n",
    "learning — the stats dataset used here was not combined with the NCAA tournament games dataset, thus each instance \n",
    "represents a Division I NCAA basketball team and their regular season statistics.\n",
    "\n",
    "## k-means\n",
    "\n",
    "As we mentioned before, the NCAA tournament divides the 64 qualified teams into 16 different seed, according to their \n",
    "performance during the regular season. In reality, the selection of tournament teams as well as seeding has a high \n",
    "degree of subjectivity. Our goals are to quantify that subjectivity using the k-means clustering algorithm.\n",
    "\n",
    "We decided to start with the data from year 2018. Initially, we sought to separate the teams into two separate clusters:\n",
    "one for qualified teams and the other for unqualified ones using the k-means algorithm. We used all 14 principal \n",
    "components (after excluding seeding) when computing the Euclidean distance in the algorithm.\n",
    "\n",
    "<img src=\"images/Regular_KMeans_AllTeams_2018.png\"/>\n",
    "<img src=\"images/AllTeams_PCA_2018.png\"/>\n",
    "\n",
    "Encouraged by the clear separation between the clusters, we decided to divide the qualified teams for the year 2018 by \n",
    "clustering the teams into 16 clusters (equivalent to 16 seeds) based on the all of the transformed statistics obtained \n",
    "with PCA. We obtained the following results\n",
    "\n",
    "<img src=\"images/Regular_KMeans_2018_16Seeds.png\"/>\n",
    "<img src=\"images/Qualified_PCA_2018_16Seeds.png\"/>\n",
    "    \n",
    "It seems like clustering was not very good. Specifically, the purity of the clustering hovers at **39.5%**. We \n",
    "hypothesized that decreasing the number of clusters to 4 (which corresponds into 1-4 seeds, 5-8 seeds, etc.) would yield\n",
    "better results\n",
    "\n",
    "<img src=\"images/Regular_KMeans_2018_4Seeds.png\"/>\n",
    "<img src=\"images/Qualified_PCA_2018_4Seeds.png\"/>\n",
    "    \n",
    "Here, purity was **56.1%**. Even though we achieved better results, we noticed another problem with the regular k-means \n",
    "algorithm, specifically that number of points per cluster is not limited. For example, the number of datapoints in the \n",
    "last run were as follows:\n",
    "\n",
    "| Cluster Number       | Number of Elements   |\n",
    "|:-----------------------------:|:----:|\n",
    "| 1                            | 10   |\n",
    "| 2                            | 15   |\n",
    "| 3                            | 17   |\n",
    "| 4                            | 22   |\n",
    "\n",
    "We would expect to see an equal number of teams assigned to each seed (or group of seeds) if following real seeding \n",
    "procedures.\n",
    "\n",
    "### Modified k-means\n",
    "\n",
    "Given these limitations, we created a modified k-means algorithm that evenly divides datapoints across clusters. \n",
    "Specifically, the clusters prioritize having the minimum distance between a datapoint and the cluster center, discarding\n",
    "the points that are farther away. These points in turn are forced to belong to another cluster. The implementation \n",
    "looked as follows for both the cases of 16 and 4 clusters.\n",
    "\n",
    "<img src=\"images/Modified_KMeans_2018_16Seeds.png\"/>\n",
    "<img src=\"images/Modified_KMeans_2018_4Seeds.png\"/>\n",
    "    \n",
    "When we looked at the purity of each clustering, we obtained slightly better results when compared to the regular \n",
    "k-means algorithm. Specifically, we obtained a purity of **41.7%** for 16 seeds (compared to 39.5%) and **57.5%** \n",
    "(compared to **56.1%**).\n",
    "\n",
    "We then decided to further test the performance of both clustering algorithms by using data for all available years in \n",
    "the dataset (2010-2018). We used the normalized statistics to minimize differences in performance across the years. \n",
    "Results of PCA are shown here for contrast against the clusters later on.\n",
    "\n",
    "<img src=\"images/Qualified_PCA_All_16Seeds.png\"/>\n",
    "<img src=\"images/Qualified_PCA_All_4Seeds.png\"/>\n",
    "    \n",
    "Furthermore, these are the results of running both algorithms for 16 clusters... \n",
    "\n",
    "<img src=\"images/Regular_KMeans_All_16Seeds.png\"/>\n",
    "<img src=\"images/Modified_KMeans_All_16Seeds.png\"/>\n",
    "    \n",
    "...and 4 clusters ...\n",
    "\n",
    "<img src=\"images/Regular_KMeans_All_4Seeds.png\"/>\n",
    "<img src=\"images/Modified_KMeans_All_4Seeds.png\"/>\n",
    "    \n",
    "In terms of purity, we obtained the following measurements.\n",
    "\n",
    "| Number of Clusters       | Regular   | Modified  |\n",
    "|:-----------------------------:|:----:|:---:|\n",
    "| **16**                            | 26.5% | 26.1% |\n",
    "| **4**                            | 54.9%   |   55.0% |\n",
    "\n",
    "\n",
    "It is important to recognize the limitations of the latter approach to unsupervised clustering. Given more data, it \n",
    "appears that both algorithms have the same performance. This spans from the prioritization of assignment of data points \n",
    "to a cluster. A careful observer would note that some points in the modified k-means algorithm are far away from their \n",
    "cluster center. This points could be called \"outliers\", not being close enough to any cluster center and thus assigned \n",
    "by default to a the center that is closest to it that doesn't have all points assigned to it yet. This method showed \n",
    "some promise, but could use refinement.\n",
    "\n",
    "## Hierarchical Clustering (Seeding)\n",
    "\n",
    "Hierarchical clustering is used to build groupings with variable cluster sizes depending on where the tree is cut. As \n",
    "the distinction between 16 seeds might be too hard, an even number of seeds can be conflated to one and the clustering \n",
    "validation can be compared for different seed sizes. PCA was applied, keeping nine components.\n",
    "\n",
    "For hierarchical clustering two important settings need to be considered. One is the metric, for which Euclidean \n",
    "distance is commonly used. The other one is the linkage, which can be single link, complete link, average link, etc. \n",
    "Here, average link was chosen because it is often recommended for general tasks, avoids chaining, and clusters mostly \n",
    "into evenly sized groups, which is necessary for our task. The effect of this can be seen in the following dendrogram.\n",
    "\n",
    "<img src=\"/images/max_dendrogram.png\" style=\"float: center margin-right: 10px;\"/>\n",
    "\n",
    "After clustering is performed, we evaluate how good it is by means of different validation metrics.\n",
    "\n",
    "<img src=\"/images/max_cluster_val.png\" style=\"float: center margin-right: 10px;\"/>\n",
    "\n",
    "The purity gives us an indicator of how purely the clusters consist of only data points from a single ground truth, with\n",
    "1 corresponding to the best purity. For the given task we get the best purity when we only partition into two seeds and \n",
    "the purity decreases with the number of seeds, as expected.\n",
    "\n",
    "The mutual information measures the amount of information shared between clustering and ground truth. The adjusted \n",
    "variant of MI is independent of the number of clusters in a partition, otherwise a higher number of clusters would give \n",
    "a better MI score. Larger values indicate a good clustering. Our results show a slight decrease in the AMI for an \n",
    "increased number of seeds.\n",
    "\n",
    "The random score is a pairwise measure, which is the fraction of true positives and true negatives over the total number\n",
    "of pairs. The adjusted rand score is centered and normalized to adjust for chance. Negative values are bad, close to \n",
    "zero means random and a score of one means that the clustering assignments are identical up to label permutations. For \n",
    "our task the ARI slightly decreases with the number of seeds.\n",
    "\n",
    "An overall trend of decreased performance can be observed as the partitioning of the teams into the seeds gets finer. \n",
    "With a purity below **20%** for partitioning into the regular 16 seeds, this means that this task is harder than \n",
    "expected based on the provided features.\n",
    "\n",
    "\n",
    "## Hierarchical Clustering (Tournament Qualification)\n",
    "\n",
    "Another clustering task is to determine which teams qualified for the tournament.\n",
    "The 3D visualization via PCA applied on the full dataset (not just qualified teams) shows that the clusters of qualified\n",
    "vs. non-qualified teams appears easier to distinguish.\n",
    "\n",
    "<img src=\"/images/max_3dpca_qual.png\" style=\"float: center margin-right: 10px;\"/>\n",
    "\n",
    "This is also visible in the decrease in variance of the principal components (when including all teams, not only \n",
    "qualified teams in PCA). Over **50%** of the variance is contained in the first component, whereas the other components \n",
    "only have minor contributions.\n",
    "\n",
    "<img src=\"/images/max_screeplot_qual.png\" style=\"float: center margin-right: 10px;\"/>\n",
    "\n",
    "We need  9 features to recover **96.2%** of the variance.\n",
    "A purity of **82%** for the qualified vs. non-qualified clustering is reached, as both clusters contain mainly points \n",
    "from one ground truth (qualified or non-qualified). But from the contigency matrix \n",
    "\n",
    "| Clustering/Qualification       | NQ   | Q  |\n",
    "|:-----------------------------:|:----:|:---:|\n",
    "| **C1**                            | 2521 | 576 |\n",
    "| **C2**                            | 28   |   0 |\n",
    "\n",
    "we can see that both clusters are assigned to the same ground truth partition. This results in a bad ARI and an AMI \n",
    "score close to 0. These results suggest that the qualified and non-qualified teams are not properly separable through \n",
    "hierarchical clustering and the distance metric utilized. The heavy bias toward non-qualified teams (as only 64 teams \n",
    "make the tournament each year) may be a contributing factor of this result.\n",
    "\n",
    "## Unsupervised Learning Conclusion\n",
    "\n",
    "The results from both k-means and hierarchical clustering seem to highlight the poor performance of unsupervised \n",
    "learning to predict the seeding positions for any given year. More complicated algorithms or more exhaustive data could \n",
    "yield to better results, but we expect those to only show marginal improvements over the algorithms described here given\n",
    "the subjectivity of seeding assignment carried out by a group of judges. This performance could indicate one of two \n",
    "different results (or a mix of the two):\n",
    "\n",
    "1. Clustering using the selected features is a poor way to group teams based on regular season performance.\n",
    "2. The selection committee does not accurately seed teams based on objective metrics of regular season performance.\n",
    "\n",
    "It is also worth noting that since 32 teams automatically qualify to the tournament based on conference championships, \n",
    "some weaker teams that happen to play in weaker conferences may qualify over stronger teams. Finally, seeding must take\n",
    "into account considerations other than just strength [6]. For example, repeats of regular season matchups are avoided to\n",
    "some extent in early rounds.\n",
    "\n",
    "# Supervised Results\n",
    "\n",
    "The following results were obtained using an 80-20 train-test split. 10-fold cross validation was performed to \n",
    "determine hyperparameters for each model using ROC AUC as the metric. The cross validations plots show performance vs\n",
    "hyperparameter values. The blue line shows CV performance, with the shaded region showing an 80% confidence interval for \n",
    "the mean performance. The orange line shows performance over the entire training set. \n",
    "\n",
    "## k-NN\n",
    "\n",
    "A standard scaler followed by PCA (with retained variance >= 0.99) were applied. 11 components are kept after the PCA \n",
    "step.\n",
    "\n",
    "<img src=\"images/longchao_knn.png\"/>\n",
    "\n",
    "The results indicate that validation ROC AUC flattens off around k=19. k=19 produced a mean CV ROC AUC of **0.784**. The\n",
    "plot shows clear overfitting at small values of k, as training performance is significantly higher than CV performance.\n",
    "\n",
    "## SVM\n",
    "\n",
    "Support vector machines use a hyperplane to separate classes, potentially using a kernel trick before separation to\n",
    "allow for non-linear classification. We landed on a radial basis function (rbf) kernel. Two important hyperparameters \n",
    "for support vector machines are C (regularization parameter) and gamma (kernel coefficient).\n",
    "\n",
    "<img src=\"/images/longchao_svm_C.png\" style=\"float: left margin-right: 10px;\"/>\n",
    "<img src=\"/images/longchao_svm_gamma.png\" style=\"float: left margin-right: 10px;\"/>\n",
    "\n",
    "A logistic regression uses a logistic function to model a binary dependent variable, resulting in a linear classifier. \n",
    "The hyperparameters to be tuned are C (inverse of the regularization strength) and tol (tolerance for stopping \n",
    "criteria). \n",
    "\n",
    "<img src=\"images/longchao_lr_C.png\" style=\"float: left margin-right: 10px;\"/>\n",
    "<img src=\"images/longchao_lr_tol.png\" style=\"float: left margin-right: 10px;\"/>\n",
    "\n",
    "The best fit parameter we have for SVM is C=100 and tol=1e-3, yielding a mean CV ROC AUC of **0.781**. One concerning\n",
    "aspect of the plots is the consistent underfitting evidenced. Training performance seems to be far lower than CV \n",
    "performance across all hyperparameter values tested.\n",
    "\n",
    "## Decision Tree\n",
    "\n",
    "Decision trees repeatedly split data based on thresholds for a certain features to classify datapoints at leaf nodes.\n",
    "Decision trees trained on data before scaling and PCA performed significantly better than after PCA, thus PCA was not\n",
    "applied for these models. The main hyperparameter of interest is the tree's maximum depth.\n",
    "\n",
    "<img src=\"images/dt_max_depth.png\"/>\n",
    "\n",
    "After cross-validation, a max-depth of three was selected, yielding a mean CV ROC AUC of **0.786**. It is clear that as \n",
    "the depth of the tree increases, the model tends to overfit, as training performance steadily rises while CV performance\n",
    "falls off after a max depth of three. Here is a tree of depth three trained on the entire training set:\n",
    "\n",
    "<img src=\"images/dt_visualization.png\"/>\n",
    "\n",
    "## Random Forest\n",
    "\n",
    "A random forest classifier is an ensemble classifier that fits many simple decision trees to the data then has them each\n",
    "vote for a class label. The implementation chosen uses bagging (bootstrap aggregating). Again, PCA seemed to hurt model \n",
    "performance and thus was not used.\n",
    "\n",
    "<img src=\"images/rf_n_estimators.png\"/>\n",
    "\n",
    "Here it appears CV performance levels off after approximately 60 estimators, with a mean CV ROC AUC of **0.799**. This\n",
    "model, however, appears to have extreme overfitting issues. Training ROC AUC (and accuracy) quickly hits **1.000**. To \n",
    "combat this issue we decided to decrease the complexity of the base estimators by limiting the max depth of the decision\n",
    "trees used to four.\n",
    "\n",
    "<img src=\"images/rf_n_estimators_simple.png\"/>\n",
    "\n",
    "Now we find 40 estimators provides the best perofrmance, with a mean CV ROC AUC of **0.814**. While this increase is\n",
    "likely not statistically significant with any reasonable degree of certainty, we did achieve the desired result — \n",
    "training performance is far closer to CV performance.\n",
    "\n",
    "## MLP\n",
    "\n",
    "A multilayer perceptron is a feedforward artificial neural network (ANN) composed of multiple layers of perceptrons with\n",
    "threshold activations. MLPs are relatively complex in terms of hyperparameters to tune.\n",
    "\n",
    "In the scikit-learn implementation, we tuned the number of hidden layers, the number of neurons in each layer, alpha (L2\n",
    "penalty parameter), initial learning rate, hidden layer activation function, and max number of iterations. With so many \n",
    "hyperparameters, a grid search based approach similar to the approaches used previously would either cover too little of\n",
    "the hyperparameter space, or take too long. Instead, a random search over the hyperparameter space was used. We limited \n",
    "the MLP to at most two hidden layers (no performance increases seen after two layers), limited neurons per layer to the\n",
    "range [5, 200], limited alpha to the range [1e-5, 1e-1] (with a log-uniform distribution), limited initial learning rate\n",
    "to the range [1e-4, 1e-2] (with a log-uniform distribution), and limited the max number of iterations to the range \n",
    "[25, 500]. Then 10000 hyperparameter configurations were run through 5-fold cross validation. The best model \n",
    "configurations from each set of 2000-3000 configurations consistently reached mean CV ROC AUC scores of approximately \n",
    "**0.86**. The chosen configuration had the highest score by a small margin, with a mean CV ROC AUC of **0.862**.\n",
    "\n",
    "| Hyperparameter | Value |\n",
    "|:--------------:|:-----:|\n",
    "| num hidden layers | 2 |\n",
    "| hidden layer sizes | (86, 24) |\n",
    "| initial learning rate | 3e-4 |\n",
    "| max iterations | 244 |\n",
    "| activation | tanh | \n",
    "\n",
    "We also examined the learning rate of the selected model.\n",
    "\n",
    "<img src=\"images/mlp_learning_curves.png\"/>\n",
    "\n",
    "## Supervised Learning Conclusion\n",
    "\n",
    "### Model Comparison\n",
    "\n",
    "| Model | Mean CV ROC AUC |\n",
    "|:-----:|:---------------:|\n",
    "| k-NN          | 0.784 |\n",
    "| SVM           | 0.781 |\n",
    "| Decision Tree | 0.786 |\n",
    "| Random Forest | 0.814 |\n",
    "| MLP           | 0.862 |\n",
    "\n",
    "As indicated above, the MLP model performed at the highest rate, with the random forest performing significantly lower. \n",
    "The remaining models each had similar, even lower performances (though these ROC AUC values were still rather high).\n",
    "\n",
    "### Testing Performance\n",
    "\n",
    "Since the MLP had the highes mean CV ROC AUC, it was selected as the model to train on the entire training set and use \n",
    "for testing. It achieved the following results on the testing set:\n",
    "\n",
    "| Metric | Score |\n",
    "|:------:|:-----:|\n",
    "| ROC AUC  | 0.888 |\n",
    "| Accuracy | 0.886 |\n",
    "| MCC      | 0.775 |\n",
    "| F1 Score | 0.885 |\n",
    "\n",
    "Using the Wilson score interval [7], we can say with **95%** certainty that our model's mean accuracy is \n",
    "**0.886 +- 0.059**.\n",
    "\n",
    "The confusion matrix produced was:\n",
    "\n",
    "| Team1 Prediction/Team1 Result | True Win | True Loss |\n",
    "|:-----------------------------:|:--------:|:---------:|\n",
    "| Predicted Win  | 51 | 9  |\n",
    "| Predicted Loss | 4  | 50 |\n",
    "\n",
    "### Performance Evaluation\n",
    "\n",
    "The official NCAA Bracket Challenge game receives millions of entries each year [8]. Unfortunately, the uncertain nature of \n",
    "matchups makes it hard to compare our model to overall accuracy in bracket challenge games. After the first round, the \n",
    "teams people predict to win games may not even get a chance to play in those games (if they were eliminated). As such,\n",
    "we can only compare accuracy with first round accuracy, however this should be a reasonable comparison. From 2015-2017, \n",
    "first round games were predicted correctly **72.1%** of the time [9], a mark well below the testing accuracy of our model. \n",
    "Additionally, of all first round matchups (1 vs. 16, 2 vs. 15, etc.) participants only predicted matchups of 1 vs. 16 \n",
    "teams at a higher rate than our model predicted all matchups (**97.9%** of 1 vs. 16 matchups were predicted correctly) \n",
    "[9]. 2 vs. 15 matchups were predicted at the same rate as our model's test, and matchups closer than 2 vs. 16 fell \n",
    "sharply in accuracy [9].\n",
    "\n",
    "Perfect predictions of NCAA tournament games will likely never be possible. Too many confounding variables are involved, \n",
    "from player injuries to motivation. Our team is very happy with the performance achieved by our final model, however. \n",
    "The model scored quite well in each metric when run on the withheld test set.\n",
    "\n",
    "# Future Work\n",
    "\n",
    "## Data\n",
    "\n",
    "The most obvious place to improve is the aquisition of more data points. While some of the advanced statistics used as \n",
    "features were not tracked until 2010, expanding the dataset to include all NCAA tournament games but reducing features \n",
    "down to basic features may still improve performance. Otherwise, as more NCAA tournament occur, more data will become \n",
    "available for training.\n",
    "\n",
    "First, a larger number of statistics could be pulled from Sports Reference as well as other sources. Metrics such as SRS\n",
    "(simple rating system), pace, RPI (rating percentage index), and true shooting percentage could all provide more \n",
    "informative power to models. Additionally, the NCAA has recently implemented a new metric called the NET (NCAA \n",
    "evaluation tool) which they claim serves as a comprehensive measure of team strength [10]. The NET ranking could not be\n",
    "used as NET rankings only date back to the 2019 season.\n",
    "\n",
    "A different approach entirely would be to use more in-depth, play-by-play statistics rather than aggregate season \n",
    "statistics. While data collection and processing would be more difficult, model performance may see benefits. This \n",
    "approach may also allow the model to account for injuries or otherwise inactive players.\n",
    "\n",
    "## Supervised Learning\n",
    "\n",
    "Utilizing random searches for all models tried may have marginally increase some of their performances by testing out a\n",
    "larger/more comprehensive hyperparameter space. In general, random search has several advantages over grid search:\n",
    "\n",
    "1. Adding hyperparameters which are uninformative does not decrease search efficiency [11].\n",
    "2. A budget can be chosen independent of the number of hyperparameters and possible values [11].\n",
    "3. Random search provides higher effective dimensionality, resulting in more efficient searches [12]."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# References\n",
    "[1] American Gaming Association (2019, Mach 18). *2019 March Madness Betting Estimates*.\n",
    "\n",
    "[2] scikit-learn. *scikit-learn: About us*. (n.d.). Retrieved from https://scikit-learn.org/stable/about.html.\n",
    "\n",
    "[2] Kvam, P., and J. S. Sokol. (2006, July 14) *A logistic regression/Markov chain model for NCAA basketball*. Naval \n",
    "Research Logistics 53(8):788–803.\n",
    "\n",
    "[4] Bunker, R. P., and F. Thabtah. (2017, September 19) *A machine learning framework for sport result prediction*. \n",
    "Applied Computing and Informatics 15(1):27–33.\n",
    "\n",
    "[5] Sports Reference. *College Basketball Statistics and History: College Basketball at Sports.* (n.d.). Retrieved from \n",
    "https://www.sports-reference.com/cbb/.\n",
    "\n",
    "[6] NCAA.com. (2019, October 31). *How the field of 68 teams is picked for March Madness.* Retrieved from \n",
    "https://www.ncaa.com/news/basketball-men/article/2018-10-19/how-field-68-teams-picked-march-madness\n",
    "\n",
    "[7] Wilson, E. B. (1927). *Probable Inference, the Law of Succession, and Statistical Inference.* Journal of the \n",
    "American Statistical Association, 22(158), 209–212. doi: 10.1080/01621459.1927.10502953\n",
    "\n",
    "[8] Wilco, D. (2020, March 26). *The absurd odds of a perfect NCAA bracket.* Retrieved from \n",
    "https://www.ncaa.com/news/basketball-men/bracketiq/2020-01-15/perfect-ncaa-bracket-absurd-odds-march-madness-dream\n",
    "\n",
    "[9] Wilco, D. (2020, April 21). Personal interview.\n",
    "\n",
    "[10] NCAA.com. (2019, December 16). *The NET, explained: NCAA adopts new college basketball ranking.* Retrieved from \n",
    "https://www.ncaa.com/news/basketball-men/article/2018-11-26/net-explained-ncaa-adopts-new-college-basketball-ranking\n",
    "\n",
    "[11] NCAA.com. *Tuning the hyper-parameters of an estimator.* (n.d.). Retrieved from \n",
    "https://scikit-learn.org/stable/modules/grid_search.html#randomized-parameter-search\n",
    "\n",
    "[12] Bergstra, J., & Bengio, Y. (2012). *Random Search for Hyper-Parameter Optimization.* Journal of Machine Learning \n",
    "Research, 13, 281–305. Retrieved from http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}