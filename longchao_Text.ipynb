{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning Algorithm Comparison ##\n",
    "During this section, we are going to show the comparison result of three different supervised learning algorithm: K-Nearest Neighbors (KNN), Support Vector Machine (SVM) and Logistic Regression. Similar to the comparison on Random Forest, Decision Tree and Multilayer Perceptron (MLP), The analysis will focus on toggling the input hyper-parameters to get the best prediction result. \n",
    "\n",
    "The dataset will use the team status of each team to predict the game result. The whole dataset will be splited into training set (80%) and test set (20%), then apply PCA (with retained variance >= 0.99) and scaling onto the training set. The training set will keep 11 components after PCA step. The dataset will now go through cross-validation to test the toggled parameters. \n",
    "\n",
    "### K-Nearest Neighbors (KNN) ###\n",
    "First algorithm to be tested is KNN. The parameter toggled here is the K-value. The K value correspond to the number of neighbors to use for the KNN queries. \n",
    "<img src=\"/images/longchao_knn.png\" style=\"float: left margin-right: 10px;\"/>\n",
    "After the cross-validation, the result showed that for a given k that is larger than 19 the accuracy will stay around 0.79 for accuracy. Applying the parameter to the test set will get a accuracy score of 0.649. This is because KNN is using the euclidean distance for the near data points calculation. If there are many features to calculate, the distance cannot represent the data set properly. Using inappropriate K values may result in over-fitting as well. \n",
    "\n",
    "### Support Vector Machine (SVM) ###\n",
    "Second algorithm to be tested is SVM. There are two important parameter for SVM input, the C (regularization parameter) and the gamma (kernel coefficient). \n",
    "<img src=\"/images/longchao_svm_C.png\" style=\"float: left margin-right: 10px;\"/>\n",
    "<img src=\"/images/longchao_svm_gamma.png\" style=\"float: left margin-right: 10px;\"/>\n",
    "The best fit parameter we have for SVM is C=1 and gamma=0.01. And the final result for SVM is 0.719 which is much better than the KNN algorithm result. \n",
    "\n",
    "### Logistic Regression ###\n",
    "Third algoritm to be tested is Logistic Regression. The parameters is similar to the SVM input. The C (inverse of the regularization strength) and the tol (tolerance for stopping criteria). \n",
    "<img src=\"/images/longchao_lr_C.png\" style=\"float: left margin-right: 10px;\"/>\n",
    "<img src=\"/images/longchao_lr_tol.png\" style=\"float: left margin-right: 10px;\"/>\n",
    "The best fit parameter we have for SVM is C=100 and tol=1e-3. And the final result for Logistic Regresssion is 0.745, similar to the SVM result. \n",
    "\n",
    "(P.S. I can wrote the result comparison part for all the supervised learning if you want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
